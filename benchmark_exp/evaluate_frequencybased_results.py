#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FrequencyBasedADpl80.csvã®çµæœè©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

def load_and_analyze_results():
    """FrequencyBasedADã®çµæœã‚’èª­ã¿è¾¼ã‚“ã§åˆ†æ"""
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    csv_path = Path("eval/metrics/uni/FrequencyBasedADpl80.csv")
    
    if not csv_path.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: {csv_path}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    
    df = pd.read_csv(csv_path)
    print(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ãƒ•ã‚¡ã‚¤ãƒ«")
    print(f"ã‚«ãƒ©ãƒ : {list(df.columns)}")
    print("\n" + "="*80)
    
    return df

def basic_statistics(df):
    """åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—"""
    print("ğŸ“Š åŸºæœ¬çµ±è¨ˆã‚µãƒãƒªãƒ¼")
    print("="*50)
    
    # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ã‚’æŠ½å‡º
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col != 'Time']  # Timeã‚’é™¤å¤–
    
    # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åŸºæœ¬çµ±è¨ˆ
    stats_df = df[numeric_cols].describe()
    
    print("\nğŸ¯ ä¸»è¦è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®çµ±è¨ˆ:")
    key_metrics = ['AUC-PR', 'AUC-ROC', 'VUS-PR', 'VUS-ROC', 'Standard-F1']
    
    for metric in key_metrics:
        if metric in stats_df.columns:
            mean_val = stats_df.loc['mean', metric]
            std_val = stats_df.loc['std', metric]
            min_val = stats_df.loc['min', metric]
            max_val = stats_df.loc['max', metric]
            median_val = stats_df.loc['50%', metric]
            
            print(f"\n{metric}:")
            print(f"  å¹³å‡: {mean_val:.4f} Â± {std_val:.4f}")
            print(f"  ä¸­å¤®å€¤: {median_val:.4f}")
            print(f"  ç¯„å›²: [{min_val:.4f}, {max_val:.4f}]")
    
    print("\nâ±ï¸ å‡¦ç†æ™‚é–“çµ±è¨ˆ:")
    time_stats = df['Time'].describe()
    print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {time_stats['mean']:.3f}ç§’")
    print(f"  ä¸­å¤®å€¤å‡¦ç†æ™‚é–“: {time_stats['50%']:.3f}ç§’")
    print(f"  æœ€å°å‡¦ç†æ™‚é–“: {time_stats['min']:.3f}ç§’")
    print(f"  æœ€å¤§å‡¦ç†æ™‚é–“: {time_stats['max']:.3f}ç§’")

def performance_analysis(df):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
    print("\nğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ")
    print("="*50)
    
    # AUC-PRã®åˆ†å¸ƒåˆ†æ
    auc_pr_scores = df['AUC-PR']
    print(f"\nğŸ“ˆ AUC-PRåˆ†æ:")
    print(f"  AUC-PR > 0.5: {(auc_pr_scores > 0.5).sum()}/{len(df)} files ({100*(auc_pr_scores > 0.5).mean():.1f}%)")
    print(f"  AUC-PR > 0.8: {(auc_pr_scores > 0.8).sum()}/{len(df)} files ({100*(auc_pr_scores > 0.8).mean():.1f}%)")
    print(f"  AUC-PR > 0.9: {(auc_pr_scores > 0.9).sum()}/{len(df)} files ({100*(auc_pr_scores > 0.9).mean():.1f}%)")
    
    # ROC-AUCã®åˆ†æ
    auc_roc_scores = df['AUC-ROC']
    print(f"\nğŸ“Š AUC-ROCåˆ†æ:")
    print(f"  AUC-ROC = 0.5 (ãƒ©ãƒ³ãƒ€ãƒ ): {(auc_roc_scores == 0.5).sum()}/{len(df)} files ({100*(auc_roc_scores == 0.5).mean():.1f}%)")
    print(f"  AUC-ROC > 0.7: {(auc_roc_scores > 0.7).sum()}/{len(df)} files ({100*(auc_roc_scores > 0.7).mean():.1f}%)")
    
    # VUS-PRã®åˆ†æ
    vus_pr_scores = df['VUS-PR']
    print(f"\nğŸ” VUS-PRåˆ†æ:")
    print(f"  VUS-PR > 0.1: {(vus_pr_scores > 0.1).sum()}/{len(df)} files ({100*(vus_pr_scores > 0.1).mean():.1f}%)")
    
    # F1-Sã‚³ã‚¢ã®åˆ†æ
    f1_scores = df['Standard-F1']
    print(f"\nâ­ Standard-F1åˆ†æ:")
    print(f"  F1 > 0.5: {(f1_scores > 0.5).sum()}/{len(df)} files ({100*(f1_scores > 0.5).mean():.1f}%)")

def dataset_category_analysis(df):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ"""
    print("\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ")
    print("="*50)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
    df['category'] = df['pdf_u'].str.extract(r'(\w+)_id_')[0]
    categories = df['category'].unique()
    
    print(f"æ¤œå‡ºã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒª: {categories}")
    
    for category in categories:
        cat_data = df[df['category'] == category]
        print(f"\nğŸ“ {category} ({len(cat_data)} files):")
        
        mean_auc_pr = cat_data['AUC-PR'].mean()
        mean_auc_roc = cat_data['AUC-ROC'].mean()
        mean_f1 = cat_data['Standard-F1'].mean()
        mean_time = cat_data['Time'].mean()
        
        print(f"  å¹³å‡AUC-PR: {mean_auc_pr:.4f}")
        print(f"  å¹³å‡AUC-ROC: {mean_auc_roc:.4f}")
        print(f"  å¹³å‡F1: {mean_f1:.4f}")
        print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {mean_time:.3f}ç§’")

def extreme_cases_analysis(df):
    """æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã®åˆ†æ"""
    print("\nğŸ” æ¥µç«¯ãªã‚±ãƒ¼ã‚¹ã®åˆ†æ")
    print("="*50)
    
    # æœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    print("ğŸ¥‡ æœ€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ (AUC-PRé †):")
    top_cases = df.nlargest(5, 'AUC-PR')
    for i, (_, row) in enumerate(top_cases.iterrows()):
        print(f"  {i+1}. {row['pdf_u']}: AUC-PR={row['AUC-PR']:.4f}, F1={row['Standard-F1']:.4f}")
    
    # æœ€ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    print("\nğŸ¥‰ æœ€ä½ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ (AUC-PRé †):")
    bottom_cases = df.nsmallest(5, 'AUC-PR')
    for i, (_, row) in enumerate(bottom_cases.iterrows()):
        print(f"  {i+1}. {row['pdf_u']}: AUC-PR={row['AUC-PR']:.4f}, F1={row['Standard-F1']:.4f}")
    
    # é•·æ™‚é–“å‡¦ç†
    print("\nâ° é•·æ™‚é–“å‡¦ç†ã‚±ãƒ¼ã‚¹ (å‡¦ç†æ™‚é–“é †):")
    slow_cases = df.nlargest(5, 'Time')
    for i, (_, row) in enumerate(slow_cases.iterrows()):
        print(f"  {i+1}. {row['pdf_u']}: {row['Time']:.1f}ç§’, AUC-PR={row['AUC-PR']:.4f}")

def overall_assessment(df):
    """ç·åˆè©•ä¾¡"""
    print("\nğŸ¯ FrequencyBasedAD ç·åˆè©•ä¾¡")
    print("="*60)
    
    # å…¨ä½“çµ±è¨ˆ
    total_files = len(df)
    mean_auc_pr = df['AUC-PR'].mean()
    mean_auc_roc = df['AUC-ROC'].mean()
    mean_f1 = df['Standard-F1'].mean()
    
    print(f"ğŸ“ˆ ç·åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
    print(f"  å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {total_files}")
    print(f"  å¹³å‡AUC-PR: {mean_auc_pr:.4f}")
    print(f"  å¹³å‡AUC-ROC: {mean_auc_roc:.4f}")
    print(f"  å¹³å‡F1-Score: {mean_f1:.4f}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
    good_performance = ((df['AUC-PR'] > 0.7) | (df['Standard-F1'] > 0.5)).sum()
    medium_performance = ((df['AUC-PR'] > 0.3) & (df['AUC-PR'] <= 0.7)).sum()
    poor_performance = ((df['AUC-PR'] <= 0.3) | (df['Standard-F1'] == 0)).sum()
    
    print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«æ•°:")
    print(f"  è‰¯å¥½ (AUC-PR>0.7 or F1>0.5): {good_performance} ({100*good_performance/total_files:.1f}%)")
    print(f"  ä¸­ç¨‹åº¦ (AUC-PR 0.3-0.7): {medium_performance} ({100*medium_performance/total_files:.1f}%)")
    print(f"  ä½èª¿ (AUC-PRâ‰¤0.3 or F1=0): {poor_performance} ({100*poor_performance/total_files:.1f}%)")
    
    # æ¨å¥¨äº‹é …
    print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
    if mean_auc_pr < 0.3:
        print("  - AUC-PRãŒä½ã„ãŸã‚ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å†ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ¨å¥¨")
    if mean_f1 < 0.2:
        print("  - F1-ScoreãŒä½ã„ãŸã‚ã€é–¾å€¤è¨­å®šã‚„ã‚¢ãƒãƒãƒªæ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã®è¦‹ç›´ã—ã‚’æ¨å¥¨")
    if (df['AUC-ROC'] == 0.5).sum() > total_files * 0.5:
        print("  - å¤šãã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ AUC-ROC = 0.5 ã®å ´åˆã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ ¹æœ¬çš„è¦‹ç›´ã—ãŒå¿…è¦")
    
    # åŠ¹ç‡æ€§è©•ä¾¡
    avg_time = df['Time'].mean()
    print(f"\nâš¡ åŠ¹ç‡æ€§è©•ä¾¡:")
    print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {avg_time:.3f}ç§’/ãƒ•ã‚¡ã‚¤ãƒ«")
    if avg_time > 10:
        print("  - å‡¦ç†æ™‚é–“ãŒé•·ã„ãŸã‚ã€è¨ˆç®—åŠ¹ç‡ã®æ”¹å–„ã‚’æ¤œè¨")
    else:
        print("  - å‡¦ç†æ™‚é–“ã¯è¨±å®¹ç¯„å›²å†…")

if __name__ == "__main__":
    print("ğŸ”¬ FrequencyBasedAD çµæœè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_and_analyze_results()
    
    if df is not None:
        # å„ç¨®åˆ†æã‚’å®Ÿè¡Œ
        basic_statistics(df)
        performance_analysis(df)
        dataset_category_analysis(df)
        extreme_cases_analysis(df)
        overall_assessment(df)
        
        print(f"\nâœ… åˆ†æå®Œäº†ï¼åˆè¨ˆ {len(df)} ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚")
    else:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
